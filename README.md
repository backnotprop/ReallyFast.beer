# reallyfast.beer ‚ö°Ô∏èüçª - for those who just want to drink their inference. Model serving as if it were done by an experienced bartender.

Auto-performance tuning and testing for ML workloads. Configure your model, push to start, wait & findout the optimal confgurations you need for production inferrence. Built for Kubernetes ML workloads.

## Why?

You've already spent ample time and energy brewing your deliocous ML model. Now you need to serve it... but you have no bar or bartender... you just have the building supplies and customer requiements. Figuring out the right architecute, and model-serving capabilities you need is a whole other conudrum from what you did in your notebook.

We are the bartender-as-a-service. Engineers

## What reallyfast.beer gives you

Our enterprise customers get the smarts of our global & continously trained ML models, as well as economies of scale discounts + multi-model and model-pipeline capabilities. For OSS users, you still get our base model, which is still wicked-smat. and this g-money library which automates some annoying performance testing tasks.

## How reallyfast.beer works

1. Clone the repo
2. Drop in your model
3. Put in some human-level requirements
   e.g. I need to scale to 10,000 users, or need not spend more than $5,000 a month
4. push to start
5. wait for the resuts

... ok its not that simple, you need a cloud env and a budget of around $100. There is some advanced config if you want to really make ths brew spicy.
